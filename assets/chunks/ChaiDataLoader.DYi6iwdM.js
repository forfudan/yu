var w=Object.defineProperty;var m=(f,e,t)=>e in f?w(f,e,{enumerable:!0,configurable:!0,writable:!0,value:t}):f[e]=t;var p=(f,e,t)=>(m(f,typeof e!="symbol"?e+"":e,t),t);const l=class l{constructor(e){p(this,"data",null);p(this,"loading",null);p(this,"dataUrl");p(this,"pakoLoaded",!1);this.dataUrl=e}async loadPako(){if(window.pako||this.pakoLoaded)return window.pako;try{const e=document.createElement("script");return e.src="https://cdnjs.cloudflare.com/ajax/libs/pako/2.1.0/pako.min.js",e.crossOrigin="anonymous",await new Promise((t,i)=>{e.onload=t,e.onerror=i,document.head.appendChild(e)}),this.pakoLoaded=!0,window.pako}catch(e){return console.warn("Failed to load pako library:",e),null}}static getInstance(e="/chaifen.json"){return l.instances.has(e)||l.instances.set(e,new l(e)),l.instances.get(e)}async loadData(){return this.data?this.data:this.loading?this.loading:(this.loading=this.fetchOptimizedData(),this.data=await this.loading,this.loading=null,this.data)}async fetchOptimizedData(){const e=performance.now(),i=[{url:this.dataUrl.replace(".csv",".json"),compressed:!0},{url:this.dataUrl,compressed:!1,csv:!0}];for(const r of i)try{const a=await fetch(r.url,{headers:{Accept:"application/json, text/csv, */*","Accept-Charset":"utf-8"}});if(!a.ok)continue;let s;if(r.csv){const c=await a.arrayBuffer(),d=new TextDecoder("utf-8").decode(c);s=this.parseCsvToOptimized(d)}else if(r.compressed){const c=await a.arrayBuffer(),d=await this.decompressGzip(c),o=new TextDecoder("utf-8").decode(d);s=JSON.parse(o)}else{const c=await a.arrayBuffer(),d=new TextDecoder("utf-8").decode(c);s=JSON.parse(d)}const n=performance.now()-e;return console.log(`📊 Loaded ${r.url} in ${n.toFixed(2)}ms`),console.log(`📦 Data contains ${Object.keys(s).length} characters`),s}catch(a){console.warn(`Failed to load ${r.url}:`,a);continue}throw new Error("Failed to load chaifen data from any source")}parseCsvToOptimized(e){const t=e.split(`
`),i={};for(let r=1;r<t.length;r++){const a=t[r].trim();if(!a)continue;const[s,n,c,d]=a.split(",");if(s){const o={};n&&(o.d=n),c&&(o.dt=c),d&&(o.r=d),i[s]=o}}return i}search(e){var r,a;if(!this.data)return console.warn("Data not loaded yet"),[];const t=[],i=e.toLowerCase();if(e.length===1&&this.data[e]){const s=this.data[e];t.push({char:e,division:s.d,division_tw:s.dt,region:s.r})}for(const[s,n]of Object.entries(this.data))(s.includes(i)||(r=n.d)!=null&&r.toLowerCase().includes(i)||(a=n.dt)!=null&&a.toLowerCase().includes(i))&&t.push({char:s,division:n.d,division_tw:n.dt,region:n.r});return t}getChar(e){if(!this.data||!this.data[e])return null;const t=this.data[e];return{char:e,division:t.d,division_tw:t.dt,region:t.r}}async decompressGzip(e){try{const t=await this.loadPako();if(t){console.log("📦 Using pako for gzip decompression");const o=new Uint8Array(e);return t.inflate(o).buffer}if(console.log("🌐 Using browser DecompressionStream for gzip decompression"),typeof DecompressionStream>"u")throw new Error("DecompressionStream not supported and pako not available");const r=new ReadableStream({start(o){o.enqueue(new Uint8Array(e)),o.close()}}).pipeThrough(new DecompressionStream("gzip")),a=[],s=r.getReader();for(;;){const{done:o,value:u}=await s.read();if(o)break;a.push(u)}const n=a.reduce((o,u)=>o+u.length,0),c=new Uint8Array(n);let d=0;for(const o of a)c.set(o,d),d+=o.length;return c.buffer}catch(t){throw console.error("Failed to decompress gzip data:",t),new Error(`Gzip decompression failed: ${t.message}`)}}};p(l,"instances",new Map);let h=l;export{h as C};
